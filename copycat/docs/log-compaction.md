---
layout: docs
project: copycat
menu: docs
title: Log Compaction
pitch: Raft architecture and implementation
first-section: log-compaction
---

<h2 id="log-compaction">8 Log Compaction</h2>

One of the most essential components of the Raft consensus algorithm is log compaction. Log compaction is the process of reducing the size of the log such that servers can continue to service writes over long periods of time while maintaining their full state on disk.

<h3 id="log-compaction-strategies">8.1 Log Compaction Strategies</h3>

The Raft literature suggests several approaches to log compaction in Raft, and we carefully considered each of them. We ultimately implemented a custom incremental log compaction algorithm in Copycat. The following sections describe the advantages and drawbacks of each of the recommended approaches to log compaction as well as an in-depth description of Copycat's custom log compaction algorithm.

<h4 id="snapshots">8.1.1 Snapshots</h4>

The canonical form of log compaction in Raft is snapshotting. Snapshotting compacts the Raft log by storing a snapshot of the state machine state and removing all commands applied to create that state. As simple as this sounds, though, there are some nuances to implementing snapshot in practice. Servers have to ensure that snapshots are reflective of a specific point in the log even while continuing to service commands from clients. This may require that the process be forked for snapshotting or leaders step down prior to taking a snapshot. Additionally, if a follower falls too far behind the leader (the follower's log's last index is less than the leader's snapshot index), additional mechanisms are required to replicate snapshots from the leader to the follower.

<h4 id="log-cleaning">8.1.2 Log Cleaning</h4>

Alternatively, Raft literature proposes log cleaning as a viable approach to log compaction. Log cleaning is the process of removing individual entries from the log once they no longer contribute to the state of the system. Diego Ongaro provides a concise description of [how log cleaning would be implemented in Raft][raft-dissertation]:

> In log cleaning, the log is split into consecutive regions called segments. Each pass of the log cleaner compacts the log using a three-step algorithm:
>
> 1. It first selects segments to clean that have accumulated a large fraction of obsolete entries.
> 2. It then copies the live entries (those that contribute to the current system state) from those segments to the head of the log.
> 3. Finally, it frees the storage space for the segments, making that space available for new segments.

The advantage of log cleaning is that it compacts the log using efficient sequential reads and writes. However, the obvious complexity if the model as described above - writing live entries to the head of the log - is that an implementation that uses this model must account for out-of-order entries in the log.

> As a result of copying the live entries forwards to the head of the log, the entries get to be out of order for replay. The entries can include additional information (e.g., version numbers) to recreate the correct ordering when the log is applied.

Because Raft is so heavily integrated with the log and its order, the prospect of having to account for order made log cleaning rather impractical for us. Additionally, certain types of state machines can be difficult to implement absent snapshots. For instance, in a counting state machine which implements *increment* and *decrement* commands, the ultimate state of the system is necessarily dependent on all the entries in the log, so all entries contribute to the state of the system as long as the counter persists. This poses a significant challenge for an abstract system like Copycat. We wanted Copycat to be able to model all types of state machines, and a pure log cleaning approach precludes that.

Additionally, there are some subtle complexities with log cleaning. It is the responsibility of the state machine to determine when an entry no longer contributes to the state machine's state, however doing so for entries that represent the removal of state from the state machine (i.e. tombstones) is more complex. An entry that removes state contributes to the system's state as long as any prior entries that contributed that state are retained in the log. This means log compaction processes must determine when it's safe to remove a tombstone from the log based on prior compaction processes.

<h4 id="log-structured-merge-trees">8.1.3 Log-structured Merge Trees</h4>

Raft literature also suggests the use of log-structured merge trees (LSM trees) for log compaction in Raft.

> LSM trees are tree-like data structures that store ordered key-value pairs. At a high level, they use disk similarly to log cleaning approaches: they write in large sequential strides and do not modify data on disk in place. However, instead of maintaining all state in the log, LSM trees reorganize the state for better random access.

LSM trees initially keep keys in a small log on disk, and once that log fills up keys are sorted and written to *runs* on disk. To service reads, trees are searched by level to locate keys in the sorted runs. Runs are periodically compacted by merging runs and removing duplicate keys.

The patterns with which LSM trees access and compact logs were certainly intriguing to us. However, considering that Copycat is not a key-value store and does not need random access to keys, significant value is lost for this application. Copycat's state machines are exclusively in-memory and are generated from replays of an ordered log, so the ability to iterate through the log in sorted order is critical to the performance of the system. If we were to use log indexes as keys in LSM trees that would largely defeat the purpose of the sorting algorithm as entries in the Raft log are inherently sorted. More notable, however, is the method with which LSM trees select runs for compaction. Runs are compacted using a generational strategy wherein more recent runs are compacted more often than older runs under the assumption that more recent writes are more likely to be duplicates.

<h3 id="log-compaction-algorithm">8.2 Log Compaction Algorithm</h3>

Each of these compaction models provide significant advantages and drawbacks. Snapshots provide the obvious benefit of simplicity from the perspective of the state machine. With snapshots, the full state machine state is simply written to disk and applied entries are removed from the log. However, snapshots can result in relatively inconsistent performance, and to some extent snapshots add complexity to the management of session events in Copycat. Log cleaning resolves potential performance issues with more incremental approach to compaction using efficient sequential reads and writes, but the added complexity of tracking liveness in the state machine can significantly reduce usability for an abstract framework like Copycat. Log cleaning also makes certain types of state machines impractical. Log-structured merge trees use interesting patterns to compact and combine runs, but the use case seems heavily geared towards random access reads from disk.

We considered each of these approaches to log compaction and ultimately opted to combine them into a custom log compaction algorithm. To meet our needs, the compaction algorithm would have to:

 * Perform incremental compaction using efficient sequential reads and writes
 * Preserve order in the Raft log after compaction to reduce complexity
 * Allow state machines to to implement log cleaning for all or a portion of entries
 * Allow state machines to implement snapshotting for all or a portion of entries
 * Reduce the cost of replication

These constraints dictated that we adopt many but not all of the concepts of log cleaning. In particular, the need to preserve order in the log precluded the use of log cleaning as it's described in the Raft literature. Thus, Copycat implements an algorithm similar to the process of log cleaning, but rather than copying live entries to the head of the log, it retains the positions and indexes of individual entries in the log after compaction.

As entries are written to the log and associated commands are applied to the state machine, state machines are responsible for explicitly releasing the commits from the log. The log compaction algorithm is optimized to select segments of the log for compaction based on the number of commits marked for removal. Periodically, a series of background threads will rewrite segments of the log in a thread-safe manner that ensures all segments can continue to be read and written. Whenever possible, neighboring segments are combined into a single segment to reduce the number of open file descriptors.

Typically, a Raft log contains entries from the point of the last compaction through the commit index and to the end of the log, but this compaction model allows entries to be missing at arbitrary points in the log. Copycat takes advantage of the added context of which entries contribute to the state machine's state to exclude obsolete entries from replication, significantly reducing the overall number of entries that need to be replicated, particularly to slower members of the cluster. However, having holes in the log is not without consequence. Copycat must account for missing entries in replication and logging. When entries are replicated to a follower, each entry is replicated with its index so that the follower can write entries to its own log in the proper sequence. Entries that are not present in a server's log or in an *AppendEntries* RPC are simply skipped in the log. In order to maintain consistency, it is critical that state machines correctly and deterministically implement processes for releasing obsolete entries from the log.

This compaction model implies that state machines must contribute to the compaction process by indicating when a command no longer contributes to the state machine's state. In practice, this can significantly increase the complexity of state machine and, as mentioned, makes certain types of state machines all but impossible to implement. However, the flexibility of the algorithm ultimately allowed us to [implement optional snapshots](#snapshots-via-log-compaction) to support a wider array of use cases while retaining the efficiency of the underlying algorithm.

<h4 id="releasing-entries">8.2.1 Releasing Entries from the Log</h4>

State machines are responsible for specifying which entries can safely be removed from the log. As entries are committed and commands are applied to the state machine, the state machine must indicate when prior commands no longer contribute to its state by *releasing* the command for compaction. All commands are retained in the log and are replicated until released by the state machine. Once released, commands may be replicated or compacted according to a configurable per-entry *compaction mode*.

For instance, if the commands `x←1` and then `x←2` are applied to the state machine in that order, the first command `x←1` no longer contributes to the state machine's state. That is, we can arrive at the final state `x=2` without applying the command `x←1`. Therefore, it's safe to remove `x←1` from the log, and because Raft guarantees that any future leader will be elected with the `x←2` command in its log, it's also safe to exclude `x←1` from further replication.

Conversely, if the command `x←nil` is later applied and released, the resulting state is not a state at all, but it's the absence of state. These types of commands - called *tombstones* - must be handled with more care. We must continue to replicate the `x←nil` tombstone even after it's released by the state machine to ensure the state is deleted on all servers. This is discussed further in the section on [handling tombstones](#major-compaction).

To track entries that no longer contribute to the state machine's state, when a state machine releases a command, the index of the associated entry is set in a memory-compact bit array used during log compaction to determine the liveness of each individual entry. That bit array is used during both [minor](#minor-compaction) and [major](#major-compaction) compaction to determine liveness for each entry in the log. Thus, the overhead of log compaction in terms of memory grows linearly with the number of entries stored on disk.

<h4 id="minor-compaction">8.2.2 Basics of Log Compaction</h4>

In order to ensure the log does not grow unbounded, a series of background tasks periodically select and rewrite segments of the log. The basic algorithm is as follows:

1. Select a set of segments to compact based on some set of configurable criteria
2. For each segment, iterate through entries in the segment file and rewrite live entries to a new segment on disk
3. Discard the old segment file

This process is known as *minor compaction* and is responsible for compacting non-tombstone entries from segments of the log. When segments are rewritten, obsolete entries are removed from the segment and live entries are copied to a new segment file on disk. This is in contrast to the log cleaning algorithm as described in the Raft literature which suggests live entries instead be copied to the head of the log. By rewriting the same segment of the log with obsolete entries removed, we ensure that the order of indexes in the log is preserved so that compacted segments can continue to be read sequentially and efficiently.

Liveness is determined by feedback from the state machine. An entry is *live* until is has been [released](#releasing-entries) by the state machine. Copycat will never remove an entry that has not been released by the state machine, but it may decline to compact an entry based on its context (for instance, if the entry is a [tombstone](#major-compaction)).

The criteria by which segments are selected for compaction can have significant impact on the overall performance of the log compaction algorithm. Ideally, segments which would result in the greatest disk space savings should be selected for compaction, but several criteria can be used to select segments according to the needs of the system. In Copycat, we use a combination of the number of times a segment has been compacted and the percentage of entries that will be removed from the segment by compaction. This is known as a generational compaction strategy and borrows from the algorithms defined by [Log-Structured Merge (LSM) trees](http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf).

Because segments are immutable and each committed segment represents a fixed range of indexes in the log, segments can be compacted in parallel during minor compaction. At the beginning of the minor compaction process, a compaction manager selects groups of segments and assigns each group to a background thread. All compaction tasks for a given compaction run must complete before the next compaction run can begin.

<h4 id="log-segment-compaction">8.2.3 Combining Log Segments</h4>

As the cluster progresses and entries are written to and removed from the log, each segment will shrink and the overall number of segments will increase. Eventually, the number of open resources can cause performance and fault tolerance issue. Therefore, in addition to reducing the size of individual segments, some mechanism is required to reduce the number of overall segments as well.

During log compaction, multiple neighboring segments are rewritten into a single segment to reduce the number of files on disk. When segments are selected for compaction, segments that are parallel to one another — such that the head of one segment flows into the tail of another segment — are given priority over disjointed segments. In Copycat, we use a generational strategy to select and combine neighboring segments within the same generation iff the resulting compact segment will be smaller than the configured maximum segment size.

<h4 id="major-compaction">8.2.4 Removing Tombstones from the Log</h4>

The minor compaction process generally is safe and efficient for removing commands that *modify* a system's state once they are no longer needed by the state machine. However, this approach does not correctly account for entries that remove state (called tombstones) from the state machine. Tombstones are essentially marker entries that indicate some set of prior entries no longer contribute to the state machine's state, including the tombstone entry itself. For instance, a command that deletes a key in a map is a tombstone. Once the delete is applied to the state machine and the key is deleted, neither the tombstone nor any prior command writing to that key contributes to the state machine's state thereafter. However, because state machine state can be rebuilt from the log, it is critical that tombstones are retained in the log as long as any prior associated entries. If a tombstone is removed from the log before the entries it invalidates, rebuilding a state machine from the log will result in an inconsistent state.

{: class="text-center"}
{% include lightbox.html href="/assets/img/docs/major_compaction.png" desc="Major Compaction" %}
*This figure illustrates the process of major compaction. The grey boxes represent entries that have been released from the log. To compact the log, the major compaction task iterates through committed segments and rewrites each segment with released entries removed (c) resulting in a compacted segment(d). The same process is then performed on the next segment (c)(d) until all committed segments have been compacted.*

Tombstones are removed from the log exclusively during a separate compaction process known as *major compaction*. The primary objective of the major compaction process is to ensure tombstones are removed in a manner that ensures failures before, during, or after the compaction task will not result in inconsistencies when state is rebuilt from the log. In order to ensure tombstones are removed only after any prior related entries, the major compaction task compacts segments in sequential order from the first index of the first segment to the last index of the last *committed* segment. This ensures that if a failure occurs during the compaction process, only entries earlier in the log will have been removed, and any tombstones that invalidate the state of those entries that appear later in the log will be retained.

In contrast to minor compaction, the major compaction process cannot run concurrently with any other compaction process. Because the major compaction process requires a full rewrite of live entries in the log, it must acquire a lock on all segments. We assume that the log consists of far fewer tombstones than non-tombstone entries, so the process is run on a fixed schedule much less frequently than minor compaction. However, tracking the total number of active tombstones and non-tombstone entries in the log could aid in determining when to perform major compaction, though Copycat doesn't do this currently. We contend that the overhead of periodically rewriting the entire Raft log is acceptable and indeed safer than the alternative, perhaps more common, approach of retaining tombstones for a predetermine amount of time.

<h4 id="log-tombstone-safety">8.2.5 Ensuring Tombstones are Applied on All Servers</h4>

The log compaction algorithm as described thus far assumes that commands can be safely removed from the log once they no longer contribute to the state machine's state. However, this is not always the case. A particular nuance in the handling of tombstones necessitates that they be applied on all servers prior to being removed from the log. If a tombstone is compacted from any server's log before it has been applied by all servers, state machines can become inconsistent. For instance, if a command deleting a particular key is submitted to a three-node cluster and committed via servers `A` and `B`, but server `C` is partitioned from the other nodes, compacting the delete from servers `A` and `B`'s logs can result in a failure to ever apply the delete on server `C` once the partition heals, thus resulting in a system where the key may or may not appear deleted based on the current leader. For example, if server `A` is the leader then the key has been deleted, but if server `C` is elected leader once the partition heals, the key will reappear.

{: class="text-center"}
{% include lightbox.html href="/assets/img/docs/log_tombstones.png" desc="Tombstone compaction" height="300px" %}
*This figure illustrates the inconsistencies that can occur if tombstones are not replicated to all servers. Entries that assign `nil` are marked as tombstones. Server `1` commits the tombstone entry `6` via servers `1` and `2` and applies the command to its state machine which releases entries `4` and `6`. But the tombstone still must be replicated to server `3`. If the tombstone is not replicated or is compacted from the leader's log, server `3`'s state machine becomes inconsistent.*

In order to ensure inconsistencies cannot result from the failure to replicate a tombstone to any server, we must ensure that each tombstone is stored on all servers before being compacted from any server's log. Some systems like Kafka handle tombstones by aging them out of the log after a fixed interval of time. Often, this can mean tombstones are held in the log for a week or more to ensure they are seen by all processes. Usually this means tombstones are retained in the log far longer than is necessary, but if the time bound is set too low, it could also mean tombstones aren't retained long enough.

Fortunately, Raft servers tend to have a more consistent and reliable view of the structure and state of the cluster than do Kafka brokers. Leaders have the context necessary to determine which followers have received a given entry, and this can be used to determine precisely when a tombstone can be safely removed from the log. This results in more efficient handling of tombstones since they can often be removed from the log much sooner, and it ensures that tombstones will always be retained at least for as long as is necessary to ensure the consistent state of the system. By keeping track of the highest index stored on all servers, the leader can indicate to followers when it's safe to remove tombstones from their logs. We call this index the `globalIndex`.

Just as with the `commitIndex`, the leader is responsible for tracking the `globalIndex` and sharing that information with followers through *AppendEntries* RPCs. The `globalIndex` is determined by the leader by simply calculating the minimum `matchIndex` for all servers in the cluster. Servers only perform major compaction to remove tombstones for segments for which all indexes are less than the `globalIndex`. This ensures that tombstones are not removed from the log until they have been stored on all servers, and once stored each server will not remove any tombstone until it has been applied to and released by the state machine.

<h4 id="major-compaction-safety">8.2.6 Preventing Race Conditions in Major Compaction</h4>

Copycat's log compaction algorithm makes no assumptions about the context within which a state machine will release an entry for compaction. Entries may be released when a session expires or once a certain amount of time has passed. This means we cannot assume once a tombstone is applied to the state machine it will be immediately released. For instance, a command that sets a time-to-live (TTL) on a key is actually a tombstone since it ultimately results in the absence of state, but it won't be released from the state machine until the amount of time specified by the TTL has expired. Because segments of the log are compacted sequentially, if the state machine continues to release entries from the log during major compaction, inconsistencies can result.

In order to ensure consistency, the major compaction process must guarantee that all prior entries associated with a tombstone have been removed from the log before the tombstone itself is removed. During the major compaction process, a compaction thread iterates through the log sequentially, rewriting segments with non-tombstone and tombstone entries removed. However, if the state machine continues to mark entries for removal from the log during the major compaction process, state machines can conceivably release a tombstone in a segment that hasn't yet been compacted and a related entry for deletion in a segment that has already been compacted, and this can result in an inconsistent state in the log.

To illustrate this point further, consider the patterns with which log compactors read segments and state machines release entries from the log. Tombstones always represent the removal of state generated by entries present earlier in the log than the tombstone itself. Because the major compaction process operates on live segments of the log to rewrite compacted segments of the log, updates to those live segments continue to occur throughout the compaction process. In other words, the state machine continues to release entries from both compacted and uncompacted segments, and the liveness of entries in segments accessible to the state machine continues to change throughout the compaction process. The evolving nature of the state of entries can result in the major compaction process removing a tombstone from the log without removing an earlier related entry, even within the same segment. Consider the following history:

 * The command `set 1` is logged at index `1` in segment `1` and applied to the state machine
 * Major compaction is initiated, and the major compaction task rewrites segment `1` with `set 1` retained in the segment
 * The tombstone `remove 1` is logged at index `12345` in segment `2` and applied to the state machine
 * The state machine then releases `set 1` at index `1` from the compacted segment `1`
 * The state machine releases `remove 1` from segment `2` since all prior related entries have been released
 * The major compaction task rewrites segment `2` without the tombstone at index `12345`

The result of the above sequence of events is a log with the `set 1` command but without the `remove 1` command. In the event that the server recovers from its log, the state machine's state will be rebuilt to `1`.

{: class="text-center"}
{% include lightbox.html href="/assets/img/docs/major_compaction_race.png" desc="Major Compaction Race" %}
*This figure illustrates how allowing state machines to continue releasing entries during major compaction can result in inconsistencies in the log. After the major compaction process writes the first segment, the state machine releases tombstones from the second segment. The major compaction process rewrites the second segment without tombstones, resulting in an inconsistent state in the log.*

Copycat resolves this issue by taking immutable snapshots of the state of released entries at the start of the major compaction process. To do so, Copycat directly copies the memory backing the bit array that holds the state of each entry in each segment. To ensure consistency, this copy is performed for the states of all entries in all segments to be compacted before any segment is compacted. Once the memory is copied, the immutable copy is used to determine the state of each entry rather than relying on the mutable bit array.

After the immutable copy of the bit array is taken, state machines still continue to update the state of entries in the live, uncompacted segment throughout the compaction process. In order to ensure the state of entries released *after* the initial snapshot is preserved in compacted segments, the final step of the major compaction process is to copy the state of remaining entries in the uncompacted segment to the compacted segment simply by iterating through the live bit array and copying release flags for entries still present in the compacted segment. Once entry states have been copied to compacted segments, the log is updated to point to the compacted segments, and the storage space for uncompacted versions of the segments is freed.

<h4 id="major-compaction-liveness">8.2.7 Liveness in Major Compaction</h4>

In order to ensure tombstones are applied on all servers, servers perform major compaction only on entries that have been replicated to all servers in the cluster. But because the progression of major compaction is dependent on replicating to the entire cluster, the failure of any single server can halt major compaction on all servers. While tombstones typically represent only a small portion of the log, we still believe it is critical that all log compaction processes be able to progress as long as a majority of the cluster is available and accepting writes.

The process of tracking the `globalIndex` to ensure tombstones are stored and applied on all servers is critical to maintaining the consistency of the system. We've demonstrated that removing tombstones from the log before they're stored on all servers [can result in significant inconsistencies](#log-tombstone-safety) in the log. In the event that a server never receives a tombstone, the only way to reliably ensure the consistency of its log is to rebuild the log from a snapshot of a server on which tombstones were correctly applied. Thus, in the event that a follower becomes unavailable for some time period, leaders must force followers to truncate their logs and reset their state once communication with the leader is reestablished.

In Copycat, leaders track availability by simply tracking the success of periodic heartbeats to followers. If a heartbeat to a follower fails repeatedly, the leader eventually commits a configuration change setting the follower's status to *unavailable*. Leaders then calculate the `globalIndex` based on the lowest `matchIndex` for all *available* followers. This means once a follower is marked unavailable, the `globalIndex` can progress beyond the end of that follower's log. Once a heartbeat to an unavailable follower succeeds, a configuration change is again committed to set the follower's status back to *available*. If the `globalIndex` progressed beyond the follower's last log index while the follower was unreachable, the follower will truncate its log on the next *AppendEntries* RPC.

Followers and passive servers determine whether and when to reset their state and log based solely on the `globalIndex` received from leaders in *AppendEntries* RPCs. Inconsistencies result when `globalIndex` is incremented beyond the highest index stored on any server. Therefore, if a server receives a `globalIndex` that is greater than its last known `globalIndex` and is greater than its local log's last index, the server must assume tombstones may have been compacted from the leader's log and thus may be missing from future replicated entries. In this case, servers truncate their log and reset their state machine's state to force replication of the leader's entire log and snapshots.

One obvious concern with this algorithm is that is can potentially result in a loss of availability while an out-of-sync server is being caught up. Servers may ultimately truncate their log only to receive a majority of what was previous present in them. However, because servers are only required to truncate their log and reset their state after they've been unavailable for some time already, this availability concern is somewhat mitigated. That is, servers are only forced to truncate their log when they've already been unreachable for some time. Nevertheless, the additional loss of availability when a partition heals may be unacceptable to some users.

Copycat provides an alternative approach to managing unreachable servers in a manner that allows major compaction to progress *and* preserves availability for the Raft cluster. The availability statuses - *available* and *unavailable* - tracked by the leader are exposed to users via an API. In the event that a server becomes unavailable, Copycat allows passive servers (which already contain the majority of the logs present in active Raft members) to be quickly promoted to replace unavailable Raft members. Then, when the leader is able to reestablish communication with an unreachable server, that server will be caught up *asynchronously* by followers while the server that replaced it participates in the Raft algorithm, thus reducing the loss of availability. The section on [membership changes](#membership-changes) goes more in-depth on the implications of membership changes and asynchronous replication.

<h4 id="log-compaction-time">8.2.8 Managing Time and Timeouts in a Compacted Log</h4>

Many Raft implementations use the replicated Raft log to provide a consistent view of time. To do so, implementations typically append the leader's timestamp to certain entries in the log. When an entry is applied to the state machine, the entry's timestamp is used to increment a monotonically increasing logical clock. This approach can be used to expire keys or sessions. Indeed, sessions as described in the Raft literature use leader timestamps to remove expired session information from memory when a client fails to send a keep-alive request to the cluster in a timely manner.

However, Copycat's log compaction algorithm introduces significantly more complexity to the process of managing time through the Raft log. Because servers logs and compaction processes progress independently of one another, state machines on different servers may see large and disparate gaps in time in the log. For example, session keep-alive RPCs are committed to the log as special keep-alive entries, and sessions are expired when the log time progresses at least a session timeout beyond the last keep-alive entry committed for a given session. This means in order to ensure sessions are not improperly expired on a replay of the log, we would have to retain every keep-alive entry from every live session. If a keep-alive entry were compacted from the log, skipping a keep-alive entry could result in a session being improperly expired by the state machine. But retaining all keep-alive entries for all live sessions is simply not practical.

In order to handle time based expirations in conjunction with log compaction, we imposed the requirement that only the leader can expire a session by committing a special entry to unregister the session. Leaders always see all log entries after the start of their term, and session expirations are always reset at the beginning of a leader's term. By committing a separate entry to unregister a session, we ensure that sessions can only be expired by the server with the most consistent view of time and sessions will never be expired as a result of missing entries that have been compacted from the log.

Nevertheless, it's clear to us that managing time and expirations through the Raft log poses significant challenges for systems that rely on log compaction. Thus, we recommend that compaction for time-based commands be managed through [snapshots](#snapshots).

<h4 id="log-compaction-membership-changes">8.2.9 Handling Membership Changes</h4>

A key component of the Raft consensus algorithm is cluster membership changes. When a new server joins the cluster, Copycat commits a configuration change adding the node to the cluster. Once the configuration change is complete, the server begins to replicate existing log entries to catch the joining server up with the rest of the cluster. But the addition of servers to the cluster poses a unique challenge for Copycat's [major compaction](#log-tombstones) process. Servers must take care to replicate globally committed entries to joining servers in a manner that ensures the resulting state of those entries is consistent.

We've already demonstrated that the failure to replicate tombstones to the entire cluster [can result in inconsistencies](#log-tombstone-safety) in the log. Similarly, when catching up a joining server, if a command is replicated to a joining server without an associated tombstone that appears later in the log, that can result in an inconsistent state for the joining server. In particular, major compaction requires that leaders keep track of the highest index to have been replicated on all servers (the `globalIndex`). This index is used to determine when it's safe to remove tombstones from the log. However, when a new server is added to the cluster, the `globalIndex` is effectively reset to `0`. Because servers are allowed to compact tombstones up to the `globalIndex`, joining servers can receive entries from servers that may or may not have compacted tombstones from their logs, thus resulting in inconsistencies when leader changes occur while a new server is being caught up. For example, if the original leader's log has not yet been compacted, it may replicate entries that set a key that is later deleted by a tombstone to the joining server. If a leader change occurs before the subsequent tombstone is replicated and the new leader has already compacted the tombstone from its log, the joining server will never receive the tombstone and thus will never delete the key set by the earlier entry.

{: class="text-center"}
{% include lightbox.html href="/assets/img/docs/global_index_unsafe.png" desc="Global index unsafe" height="400px" %}
*This figure illustrates a case wherein replicating released entries to a joining server can result in inconsistent logs. The grey boxes represent entries that have been released but not removed from the log, and the white boxes represent entries that have been both released and removed from the log. `S1` begins replicating entries to the new member, `S4`, and crashes after replicating entries up to index `6`. `S2` is then elected leader and continues replicating entries to `S4`. However, because `S2` compacted entries from its log before `S1`, `S1` ended up sending entries to `S4` that were already removed from `S2`. This results in `S1` sending entries that will never be compacted on `S4` and thus results in an inconsistent state.*

There are several potential approaches to countering the inconsistencies that can result from leader changes while catching up a joining server. The first solution we considered for handling leader changes when catching up a new server was forcing a joining server to simply truncate its log when a leader change occurs. This would ensure that a joining server's log precisely mirrors that of a single server at least up to the `globalIndex`. While certainly feasible, this was not ideal primarily for performance reasons. An unstable cluster could force a joining server to repeatedly truncate its log through no fault of its own, and the failure to catch up a new server can ultimately cause availability issues.

The inconsistencies that result from leader changes while catching up a new server are a direct result of replicating entries that no longer contribute to the state machine's state and have thus been marked for removal from the log. Ultimately, replication of these entries is inconsequential to the final state of the system, so a more elegant solution to this consistency problem lies in how those entries are read from the log and replicated to joining servers. If we simply exclude globally replicated and released entries from *AppendEntries* RPCs, we can ensure joining servers never receive entries that are removed by a tombstone later in the globally replicated log. When selecting entries for replication to a joining server, if an entry's `index` is less than the `globalIndex`, that entry will be exluded from replication. This has the additional benefit of improving the performance of catching up new servers by replicating only the minimum number of entries necessary to ensure consistency.

{: class="text-center"}
{% include lightbox.html href="/assets/img/docs/global_index_safe.png" desc="Global index safe" height="400px" %}
*This figure illustrates how servers replicate logs without released entries to members joining the cluster. By excluding released entries replicated to a new server, the new server cannot receive globally committed entries that would otherwise have been compacted given the full context of the log, so switching servers is safe.*

Considering that Copycat prevents released entries from being replicated to joining servers, we examined whether the loss of context in the log could cause consistency issues for joining servers. In particular, we wondered if the same [race condition](#major-compaction-safety) we see in major compaction could be duplicated in this scenario. Because there are no rules for when state machines can release entries - except that the processes for releasing entries must be deterministic - it is conceivable that a leader could replicate a live entry that is later made obsolete by a tombstone and the server could crash before replicating that associated tombstone. However, because Raft ensures that any new leader will be elected with all committed log entries, and because all newly committed tombstones will always be present and replicated to the joining server, that ensures that a new leader's globally replicated log will be based on the same post-join state as the original leader. If a new tombstone is committed to the leader and releases a globally replicated entry, a joining server will still receive that tombstone.

<h4 id="replication-performance">8.2.10 Replication Performance and Log Compaction</h4>

The implementation of [safety fixes for membership changes](#log-compaction-membership-changes) in log compaction led to the realization that Copycat's log compaction algorithm could be used to significantly improve performance in replication by excluding entries that don't need to be replicated to certain servers. Because a command can be applied to the state machine once stored on a majority of servers, there is some opportunity for a leader's state machine to release an entry from the log prior to replication to a minority of the nodes in the cluster. Often, that minority may represent the slowest group of servers, so there may be significant benefit to reducing the total number of entries that need to be replicated to those servers.

Because Raft ensures that any server that's elected leader will have all committed entries from the prior term, and because state machines release entries deterministically and can only release an entry once it has been committed, some entries can be safely excluded from replication once they've been released from the state machine. In other words, if one leader's state machine has released an entry, Raft ensures that any subsequent leader's state machine will have done the same, so excluding those entries altogether from replication is safe in many cases.

Copycat controls which entries need to be replicated on a per-entry based on the state of the entry and the entry's *compaction mode*. Once an entry has been marked for removal, it's not necessarily the case that the entry can be excluded from all replication. [Tombstones](#log-tombstones) and entries that are otherwise required to be stored on all servers and where the `index` of the entry is greater than the `globalIndex` are always included in replication. [Snapshottable](#log-compaction-snapshots) entries are replicated until a snapshot with an `index` greater than the entry index has been taken and persisted. All other entries are excluded from replication once released by the state machine.

<h3 id="snapshots-via-log-compaction">8.3 Implementing Snapshots Via Log Compaction</h3>

Snapshots are the canonical form of log compaction in Raft. They work by periodically writing the complete state machine state to a snapshot file on disk and then removing all entries up to the last index applied to the state machine. While snapshots can impact performance, they do provide a significantly simpler approach for managing system state with respect to log compaction. Indeed, many types of state machines necessitate snapshotting. For instance, a Raft log with entries representing *increments* and *decrements* in a counter cannot be compacted using Copycat's normal compaction algorithm because ultimately the state of the counter is the sum of all increments minus all decrements. We could compact *some* of the entries from the log based on the difference of increments and decrements, but if the counter is ever only incremented then the log will still effectively grow unbounded. Snapshots significantly reduce the complexity of state machines by removing the need to track the *liveness* of commands applied to the state machine, and in the case of counters they provide a significantly more efficient alternative to storing state changes. State machines simply write the counter value to disk, and in the event of a replay of the log that snapshot is used to recover the system's state prior to the first entry in the log.

There are clear advantages to snapshotting, so we wanted to explore ways to integrate them into Copycat's existing log compaction algorithm. Fortunately, Copycat's log compaction algorithm does not prohibit the use of snapshots. To implement support for snapshots, we simply built on top of the existing algorithm to abstract the process of releasing entries from a snapshottable state machine. When a segment fills up on disk, Copycat requests a snapshot from the state machine. Once the snapshot is written to disk, all prior entries in the log are automatically marked for removal during log compaction.

<h4 id="log-compaction-snapshots">8.3.1 Combining Snapshots and Log Compaction</h4>

Out of necessity, Copycat's log handles compaction on a per-entry basis. Compaction processes remove certain entries based on several factors including the type of entry and whether the entry has been released by the state machine. *Snapshottable* entries are similarly handled in a unique manner. When a snapshot of the state machine's state is taken and persisted, snapshottable commands are automatically marked for removal from the log. Because of the level of granularity - state machines can snapshot specific entries - state machines can use a mixture of snapshotting and log compaction to manage their state on disk. For instance, a keyed state machine might use the standard log compaction approach to mark overwritten commands for removal from the log, while a counting state machine might use snapshots to persist the counter state at specific points in the log. A state machine that does both can simultaneously take advantage of the performance of incremental log compaction for a large number of keys and avoid the complexity inherent in tracking liveness for certain types of commands.

<h4 id="snapshot-session-events">8.3.2 Managing Session Events for Snapshotted Logs</h4>

Commands that are written to the Copycat log and applied to the state machine can trigger [event messages](#session-events) which are sent by the state machine to clients. Event messages are stored in memory on each server for fault-tolerance, and in the event of a replay of the log unacknowledged events must be regenerated from entries on disk. This particularly affects the process of snapshotting. If a snapshot is taken of the state machine state and snapshotted entries are immediately removed from disk before being received by clients, a failure of the server can result in a loss of the event messages. Entries that generate session events must must be retained in the log at least until associated session events have been received by all clients.

There are a few potential solutions to ensure event messages are not lost in the event of a failure after snapshotting the log. The first and most obvious solution is to store event messages in the snapshot itself. Indeed, storing events in the snapshot seems intuitive, but in practice events are short-lived and are typically received and acknowledged by clients within less than a single heartbeat, so long-term persistence of event messages represents unnecessary overhead.

Alternatively, snapshottable commands could be retained in the log to regenerate session events upon replay of the log, but because a snapshot has already been taken for those commands, the commands no longer contribute to the state machine's state. However, this would require that state machines be idempotent and ensure that associated session events are always published, and we feel that would be rather impractical.

Instead, Copycat takes an alternative approach to ensuring session events can be regenerated from logs directly simply by awaiting the acknowledgment of pending events before persisting a snapshot. When a snapshot of the state machine's state is taken, the snapshot is stored on disk but is not immediately marked as completed, and prior snapshots are retained. Once events produced by commands up to the snapshot index have been acknowledged by clients, the snapshot is marked as completed and storage for preceding snapshots is freed. When the server recovers from disk, the snapshot is installed and only entries that follow the snapshot index may generate session event messages.

{% include common-links.html %}
